# Vision-Based Target Tracking Robot

**Autonomous TurtleBot3 that uses computer vision and LIDAR to find and follow targets.**

---

## ðŸš€ How It Works (Simple Explanation)

This robot operates like a simple autonomous creature:

1.  **Seeing (Vision)**: It uses a front-mounted camera to look for **Cyan/Teal** objects. It ignores everything else (like walls or other robots).
2.  **Sensing (LIDAR)**: It uses a laser scanner to measure exactly how far away objects are, ensuring it doesn't crash.
3.  **Thinking (Control)**:
    *   **If it sees the target**: It turns to center it horizontally while approaching.
    *   **Edge-based distance control**: Instead of using fixed distances, it approaches until the target edges are near the frame border, then stops.
    *   **If target edges clip**: Backs up to keep the full target visible.
    *   **If it loses the target**: Spins in place to search for it.

**Smart Distance Control:**
The robot uses a two-tier margin system to get as close as possible without the target leaving the camera frame:
- **15px Safety Margin**: Stops approaching when target edges are within 15 pixels of frame border
- **2px Clip Margin**: Backs up if target edges actually reach the frame border (clipped)
- **Hold Zone**: Between these margins, the robot holds steady without oscillating

This intelligent approach maximizes proximity while ensuring the target stays fully visible at all times.

---

## âš¡ Quick Start

**1. Build the Project**
```bash
cd ~/AI-Robotics-Group-12-Project
colcon build
source install/setup.bash
```

**2. Run the Simulation (Recommended)**
```bash
./run_with_camera_viewer.sh
```
This single command launches both Gazebo simulation and the camera viewer!

**Alternative - Manual Launch:**
```bash
# Terminal 1: Launch simulation
ros2 launch lidar_target_follower obstacle_course.launch.py

# Terminal 2: View camera feed
python3 view_camera.py
```

**3. Interact**
*   **Move the Target**: In Gazebo, use the "Translate" tool to drag the cyan cylinder. The robot will actively follow and maintain distance.
*   **Camera Viewer**: Shows live feed with target detection, contours, crosshair, and tracking info.
*   **FOV Visualization**: The robot displays yellow glowing lines in Gazebo showing the camera's 60Â° field of view cone.

---

## ðŸ”§ Technical Architecture

### Hardware & Sensors
*   **Robot Platform**: TurtleBot3 Burger (Differential Drive)
*   **Vision Sensor**: RGB Camera (640x480 @ 30Hz)
*   **Distance Sensor**: GPU LIDAR (360Â° scan @ 5Hz, 3.5m range)

### Software Stack
*   **OS**: Ubuntu 22.04 LTS
*   **Middleware**: ROS2 Humble
*   **Simulation**: Ignition Gazebo Fortress
*   **Vision Library**: OpenCV 4.5.4 (via `cv_bridge`)

### Control Logic (Priority System)
The robot makes decisions 20 times per second (20Hz) based on this hierarchy:

1.  **EMERGENCY STOP**: If *any* object is < 0.30m â†’ STOP immediately (collision avoidance).
2.  **EDGES CLIPPED**: If target edges are within 2px of frame border â†’ BACK UP at -0.20 m/s.
3.  **TARGET TOO FAR**: If target fills < 20% of FOV â†’ APPROACH FAST at 0.30 m/s.
4.  **TARGET SAFE & SMALL**: If edges safe (>15px from border) AND fills < 60% FOV â†’ APPROACH MEDIUM at 0.15 m/s.
5.  **TARGET SAFE & LARGE**: If edges safe AND fills â‰¥ 60% FOV â†’ HOLD POSITION (stabilize).
6.  **EDGES IN DANGER ZONE**: If edges between 2-15px from border â†’ HOLD POSITION (prevent oscillation).
7.  **TARGET OFF-CENTER**: If target not centered â†’ TURN + APPROACH MEDIUM at 0.15 m/s with proportional control + size-based damping.
8.  **NO TARGET**: If nothing detected â†’ SEARCH by rotating at 0.4 rad/s.

**Turn Control:**
- Proportional control: `angular_vel = -horizontal_error * gain`
- Size-based damping: Reduces sensitivity when target is close (prevents over-correction)
- Turn deadzone: Â±0.05 (prevents micro-adjustments)

**Edge Detection System:**
- **Safety Margin (15px)**: Creates a "keep-out zone" near frame borders
- **Clip Margin (2px)**: Detects when target is actually clipping the frame
- **Visibility Status**: Tracks whether all 4 edges (left, right, top, bottom) are safely visible
- **Prevents Oscillation**: Robot won't flip-flop between approach/backup at the boundary

### Vision Pipeline
1.  **Input**: Raw RGB image from camera (640Ã—480).
2.  **Preprocessing**: Convert BGR to HSV color space (better for color detection).
3.  **Filtering**: Threshold for Cyan (Hue: 80-100, Saturation: 50-255, Value: 50-255).
4.  **Cleanup**: Apply morphological operations (5Ã—5 ellipse kernel):
    - Close: Fill small holes inside target
    - Open: Remove small noise outside target
5.  **Detection**: Find contours using `cv::findContours()`.
6.  **Selection**: Choose largest contour (most likely the target).
7.  **Bounding Box**: Calculate `cv::boundingRect()` for edge positions.
8.  **Edge Analysis**: Check if each edge (left, right, top, bottom) is:
    - Within safety margin (15px from border)
    - Clipped (within 2px from border)
9.  **Output**: 
    - Normalized X-coordinate for steering (-1 = left, +1 = right)
    - Width/Height ratios (target size as fraction of frame)
    - Edge visibility status (safe, danger zone, or clipped)

### Modern HUD Camera Viewer
The `view_camera.py` script displays real-time telemetry with a user-friendly interface designed for non-technical users:

**Features:**
- **"What is the Robot Doing?" Panel** (top):
  - Simple status icons: MOVING, REACHED, SEARCHING, BACKING UP, IDLE
  - Plain English explanations (e.g., "Getting closer to target", "Too close! Stopping.")
  - Visual speed bar showing movement intensity
  - Color-coded status (Green=Good, Cyan=Moving, Orange=Searching, Red=Warning, Magenta=Backing)
  
- **Target Information Panel** (bottom):
  - "TARGET FOUND" or "NO TARGET DETECTED" status
  - Distance to target in meters
  - Size in view (% of frame coverage)
  - Alignment status with visual indicators:
    - Arrows showing turn direction when off-center
    - Circle indicator [OK] when centered
  
- **Visual Elements**:
  - Center crosshair for alignment reference
  - Target bounding box with corner brackets
  - Frame corner accents for modern look

**Design:**
- Dark semi-transparent panels for readability
- Color-coded status for quick understanding
- No technical jargon - friendly for all users
- Visual indicators (bars, arrows, shapes) instead of complex numbers
- Left-side consolidated layout keeps view unobstructed

---

## ðŸ“‚ Project Structure

```text
AI-Robotics-Group-12-Project/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ vision_target_follower_node.cpp  # Main C++ control node with edge-based distance control
â”œâ”€â”€ launch/
â”‚   â””â”€â”€ obstacle_course.launch.py        # Launches Gazebo, Robot, and Nodes
â”œâ”€â”€ models/                              # SDF Models
â”‚   â”œâ”€â”€ turtlebot/                       # Robot with Camera + LIDAR
â”‚   â””â”€â”€ cyan_target/                     # The target cylinder
â”œâ”€â”€ worlds/
â”‚   â””â”€â”€ obstacle_course.world            # Simulation environment
â”œâ”€â”€ view_camera.py                       # Modern HUD camera viewer
â”œâ”€â”€ run_with_camera_viewer.sh            # One-command launch script
â””â”€â”€ CMakeLists.txt                       # Build configuration
```

---

## ðŸ› ï¸ Installation & Setup

**Prerequisites**: Ubuntu 22.04, ROS2 Humble, Gazebo Fortress.

**1. Install Dependencies**
```bash
sudo apt update
sudo apt install ros-humble-ros-gz-sim ros-humble-ros-gz-bridge \
                 ros-humble-cv-bridge libopencv-dev
```

**2. Setup Workspace**
```bash
mkdir -p ~/AI-Robotics-Group-12-Project/src
# Clone or copy files here
```

**3. Build**
```bash
colcon build --symlink-install
```

---

## â“ Troubleshooting

| Issue | Solution |
|-------|----------|
| **Robot spins in circles** | It cannot see the target. Ensure the Cyan Cylinder is in the world. |
| **Robot hits the wall** | LIDAR might be blocked. Check if `scan` topic is publishing. |
| **Build fails** | Ensure `cv_bridge` and `OpenCV` are installed (`sudo apt install libopencv-dev`). |
| **Gazebo crashes** | Try killing processes: `pkill -f "ign gazebo"`. |
